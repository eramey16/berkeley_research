{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test_lgs_params.ipynb\n",
    "### attempting to pull relevant parameters from science and weather data\n",
    "### imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from astropy.table import Table\n",
    "from astropy.io import fits\n",
    "import lgs_metadata_compiler as md\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from importlib import reload\n",
    "### ML algorithms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Files and folders\n",
    "root_dir = \"/g/lu/data/gc/\" # root data directory\n",
    "#tables = \"/u/steverobinson/work/keckao/\" # folder of data tables found in Steve's directory\n",
    "data_dir = \"data/\"\n",
    "meta_file = data_dir+\"keck_metadata.dat\" # test metadata table\n",
    "# strehl and fwhm are labels, the rest are features\n",
    "use_cols = ['strehl', 'fwhm', 'airmass', 'az', 'mass', 'dimm', 'wind_speed', \n",
    "               'wind_direction', 'temperature', 'relative_humidity', 'pressure']\n",
    "Y_data = use_cols[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read info from Steve's metadata files\n",
    "#meta_data = Table.read(meta_file)\n",
    "#meta_data = meta_data.to_pandas() # convert to pandas\n",
    "meta_data = pd.read_csv(meta_file)\n",
    "meta_data.columns = [c.lower() for c in meta_data.columns]\n",
    "print(meta_data.shape)\n",
    "\n",
    "# extract relevant data\n",
    "meta_data = meta_data[use_cols]\n",
    "# clean data\n",
    "meta_clean = util.clean(meta_data, dropna=True)\n",
    "print(meta_clean.shape)\n",
    "\n",
    "### Graph the features to see what values they take on\n",
    "bins = 50\n",
    "\n",
    "for i, col in enumerate(use_cols):\n",
    "    fig, ax = plt.subplots()\n",
    "    '''\n",
    "    # manually define bin edges for wind direction\n",
    "    if col==\"wind_direction\":\n",
    "        wd_filter = meta_data[col][meta_data[col] > -100]\n",
    "        bin_edges = np.linspace(min(wd_filter), max(wd_filter), bins-1)\n",
    "        bin_edges = np.insert(bin_edges, 0, [-10000, -9990])\n",
    "        \n",
    "        # Break axes\n",
    "        divider = make_axes_locatable(ax[i])\n",
    "        ax2 = divider.new_horizontal(size=\"300%\", pad=0.2)\n",
    "        fig.add_axes(ax2)\n",
    "        \n",
    "        # Plot data\n",
    "        ax[i].hist(meta_data[col], bins=bin_edges) # outlier\n",
    "        ax2.hist(meta_data[col], bins=bin_edges, label=\"before\") # data\n",
    "        \n",
    "        # Clean up\n",
    "        ax[i].set_xlim([-11000, -9900])\n",
    "        ax[i].spines['right'].set_visible(False)\n",
    "        ax[i].tick_params(right=\"off\", labelright=\"off\")\n",
    "        ax2.set_xlim([0, 1000])\n",
    "        ax2.tick_params(left=\"off\", labelleft=\"off\")\n",
    "        ax2.spines['left'].set_visible(False)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "    #else: # normal bin edges\n",
    "    _, bin_edges, _ = ax.hist(meta_data[col], bins, label=\"before\")\n",
    "    \n",
    "    # Clean histogram\n",
    "    ax.hist(meta_clean[col], bins=bin_edges, histtype='step', label=\"after\")\n",
    "    \n",
    "    ax.set_xlabel(col+\" value\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.legend(loc='best')\n",
    "    plt.savefig(\"plots/\"+col+\"_hist.png\", bbox_inches=\"tight\")  \n",
    "    \n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"plots/feature_histograms.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract y variables\n",
    "Y = meta_clean[use_cols[:2]]\n",
    "strehl = meta_clean['strehl']\n",
    "fwhm = meta_clean['fwhm']\n",
    "\n",
    "# Extract x variables\n",
    "X = meta_clean[use_cols[2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scale the data\n",
    "# StandardScaler() subtracts the mean and divides by the standard deviation\n",
    "# Very important for PCA to normalize for high-magnitude features\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "#X_scaled = X\n",
    "# (1-tolerance) is the amount (%) of variation we keep in our data\n",
    "tolerance = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic PCA - extracts features which have the most variation in the data\n",
    "pca = PCA(1-tolerance)\n",
    "#pca = PCA(n_components=3)\n",
    "pca.fit(X_scaled)\n",
    "# View variance ratios\n",
    "list(np.round(pca.explained_variance_ratio_, 2))\n",
    "# 1. If the emphasis is on knowing what our features mean then this is not a great algorithm to use\n",
    "# 2. We have so few features that we probably don't need to eliminate any if we use them as-is\n",
    "# 3. The explained variance ratios are also all fairly small, so PCA isn't likely to help a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot eigenvalues\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel(\"Component\")\n",
    "plt.ylabel(\"Explained variance\")\n",
    "plt.savefig(\"plots/pca_expl_var.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot makeup of principal components\n",
    "x = np.arange(0, pca.n_components_)\n",
    "colors = plt.cm.get_cmap('viridis', pca.n_components_)\n",
    "\n",
    "prev_bar = np.zeros(pca.n_components_)\n",
    "for i, component in enumerate(pca.components_.T):\n",
    "    c = component**2\n",
    "    plt.bar(x, c, width=0.5, bottom=prev_bar, label=use_cols[2:][i])\n",
    "    prev_bar += c\n",
    "\n",
    "lgd = plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.xlabel(\"Component\")\n",
    "plt.ylabel(\"Percent used\")\n",
    "plt.title(\"Composition of Principal Components\")\n",
    "plt.savefig(\"plots/pca_composition.png\", bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {}\n",
    "for col in Y.columns:\n",
    "    y = Y[col]\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X,y, random_state=123)\n",
    "    errors[col] = test_y\n",
    "    errors[col+\"_X\"] = test_X\n",
    "    \n",
    "    rf = RandomForestRegressor(n_estimators=100)\n",
    "    rf.fit(train_X, train_y)\n",
    "    train_pred = rf.predict(train_X)\n",
    "    test_pred = rf.predict(test_X)\n",
    "    \n",
    "    # Calculate errors\n",
    "    train_err = (train_pred-train_y)\n",
    "    test_err = (test_pred-test_y)\n",
    "    train_mape = np.abs(train_err)/train_y\n",
    "    test_mape = np.abs(test_err)/test_y\n",
    "    \n",
    "    errors[col+\"_mape_train\"] = train_mape\n",
    "    errors[col+\"_mape_test\"] = test_mape\n",
    "    errors[col+\"_err_train\"] = train_err\n",
    "    errors[col+\"_err_test\"] = test_err\n",
    "    \n",
    "    print(f\"{col}:\\ntraining total: {np.mean(np.abs(train_err))}, testing total: {np.mean(np.abs(test_err))}\\ntraining %: {np.mean(train_mape)*100}, testing %: {np.mean(test_mape)*100}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors[\"strehl\"], np.abs(errors[\"strehl_mape_test\"]), '.')\n",
    "plt.xlabel(\"Strehl\")\n",
    "plt.ylabel(\"Strehl mape error\")\n",
    "#plt.savefig(plot_dir+\"strehl_naive_rf_err_v_strehl.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(errors[\"fwhm\"], np.abs(errors[\"fwhm_mape_test\"]), '.')\n",
    "plt.xlabel(\"FWHM\")\n",
    "plt.ylabel(\"FWHM mape error\")\n",
    "#plt.savefig(plot_dir+\"fwhm_naive_rf_err_v_fwhm.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(errors[\"strehl_X\"].columns),2, figsize=[10,20])\n",
    "for i,col in enumerate(errors[\"strehl_X\"].columns):\n",
    "    ax[i,0].scatter(errors[\"strehl_X\"][col], np.abs(errors[\"strehl_mape_test\"]))\n",
    "    ax[i,0].set_xlabel(col)\n",
    "    ax[i,0].set_ylabel(\"strehl error\")\n",
    "    ax[i,1].scatter(errors[\"fwhm_X\"][col], np.abs(errors[\"fwhm_mape_test\"]))\n",
    "    ax[i,1].set_xlabel(col)\n",
    "    ax[i,1].set_ylabel(\"fwhm error\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing Random Forest Regressor\n",
    "\n",
    "# Set up parameters and data sets\n",
    "all_data = meta_clean.copy()\n",
    "all_data['strehl_pred'] = np.nan\n",
    "all_data['fwhm_pred'] = np.nan\n",
    "all_data['ML_indicator'] = np.nan\n",
    "\n",
    "params = {\"n_estimators\":np.arange(5, 500, 10), \"max_depth\": np.arange(5, 100, 5), \n",
    "          \"max_features\":np.arange(1, len(X.columns)+1)}\n",
    "\n",
    "# Seed random generator\n",
    "np.random.seed(123)\n",
    "# train-test split\n",
    "train_X, test_X = train_test_split(X)\n",
    "train_idxs = train_X.index.values\n",
    "test_idxs = test_X.index.values\n",
    "\n",
    "for col in Y.columns:\n",
    "    # Get y data\n",
    "    y = Y[col]\n",
    "    train_y = y[train_idxs]\n",
    "    test_y = y[test_idxs]\n",
    "\n",
    "    # Initialize Random Forest Regressor\n",
    "    rf = RandomForestRegressor()\n",
    "    search = GridSearchCV(rf, params, cv=5, scoring=\"neg_mean_absolute_error\")\n",
    "    best = search.fit(train_X, train_y)\n",
    "    all_data[col+\"_pred\"][train_idxs] = best.predict(train_X)\n",
    "    test_data[col+\"_pred\"][test_idxs] = best.predict(test_X)\n",
    "    print(all_data)\n",
    "    \n",
    "    # Calculate errors\n",
    "    train_err = (train_pred-train_y)/train_y\n",
    "    test_err = (test_pred-val_y)/val_y\n",
    "    train_mape = np.mean(np.abs(train_err))\n",
    "    test_mape = np.mean(np.abs(test_err))\n",
    "\n",
    "rf_errs_train = pd.DataFrame()\n",
    "rf_errs_test = pd.DataFrame()\n",
    "for key in percent_errors:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots\n",
    "# Strehl\n",
    "plt.hist(100*percent_errors['strehl'], bins=100)\n",
    "plt.title(\"Strehl fit\")\n",
    "plt.xlabel(\"Percent error\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.savefig(\"plots/strehl_naive_rf_err.png\")\n",
    "plt.show()\n",
    "\n",
    "# FWHM\n",
    "plt.hist(100*percent_errors['fwhm'], bins=100)\n",
    "plt.title(\"FWHM fit\")\n",
    "plt.xlabel(\"Percent error\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.savefig(\"plots/fwhm_naive_rf_err.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
